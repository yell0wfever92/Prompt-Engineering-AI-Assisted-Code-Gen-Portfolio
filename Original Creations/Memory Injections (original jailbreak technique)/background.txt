I coined the term "memory jailbreak" in the context of bypassing LLM restrictions via injecting prompts into the bio tool with the first known bypass method presented to the r/ChatGPTJailbreak subreddit on May 13th, 2024. Check that post out here: https://www.reddit.com/r/ChatGPTJailbreak/comments/1crc4pw/memory_jailbreak_works_for_gpt4o/

I have since used memory injections to jailbreak any LLM that offers it as a feature and have several projects in the works which leverage them. The Master Key and CompDoc are memory injections, and I have also shared techniques to add entire custom GPT jailbreaks (like Professor Orion) to GPT's memory, thereby allowing the default model to be a jailbreak in each chat. Check out examples of this here, which also utilize json formatting to make it appear legitimate to the AI: https://www.reddit.com/r/ChatGPTJailbreak/comments/1dwlxum/tobio_jailbreak_enhancement_directly_edit_your/